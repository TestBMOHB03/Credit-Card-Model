{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMO Credit Card Use Case 2 Modelling: 1M Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bit of set up to start with...  Let's make sure skflow is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install skflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gcp\n",
    "import gcp.bigquery as bq\n",
    "import skflow\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personal preference but I like to wind these configuration settings up because we work with some wide DataFrames and I like to see all the column headings.  Similar argument for rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 2: Method of Payment of Card Balances\n",
    "\n",
    "From the use case description document...\n",
    "\n",
    "* This scenario purports to detect AML risks in which a money launder may be using money from hard-to-trace source to pay off credit card balance.\n",
    "* Categorize the credit payments by the payment methods and look for unusual patterns, e.g. consistent or large usage of ACH by a domestic customer to pay off the credit card\n",
    "* The following fields should be part of transaction data schema.\n",
    "\n",
    "|Transaction Type|Credit_Debit_Flag|\n",
    "|---|---|\n",
    "|Payment|C (Credit)|\n",
    "|Refund|C|\n",
    "|Reversal|C|\n",
    "|Award|C|\n",
    "|Purchase|D (Debit)|\n",
    "|Fee|D|\n",
    "|Interest Charge|D|\n",
    "|Penalty|D|\n",
    "\n",
    "|Payment Method|\n",
    "|---|\n",
    "|Cash|\n",
    "|Wire|\n",
    "|ACH|\n",
    "|Paper Check|\n",
    "|e-Check|\n",
    "|Online Transfer|\n",
    "|Payment at ATM|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we'll take from this is the simplest possible interpretation...  We'll look at credits made to accounts and see if we can infer fraud from the patterns in those credits.  N.B. We'll ignore all other factors in this first version although we woudn't in later versions.  This one is for illustrative purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (Abbreviated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's get a feel for the data...\n",
    "\n",
    "We'll pull in the data for Use Case 2 and pivot it to give us a count for each variant of the use case (Green, Yellow, Red) of the number of transactions of each credit action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module use_case_2\n",
    "\n",
    "DEFINE QUERY trans_by_type\n",
    "SELECT\n",
    "  USE_CASE as use_case,\n",
    "  TRANSACTION_TYPE AS trans_type,\n",
    "  COUNT(*) AS num_trans\n",
    "FROM\n",
    "  [CreditCardv2.cc_trans_02052016]\n",
    "WHERE\n",
    "  REGEXP_MATCH(USE_CASE, 'Use Case 2')\n",
    "  AND CREDIT_DEBIT == 'C'\n",
    "GROUP BY\n",
    "  trans_type, use_case\n",
    "ORDER BY\n",
    "  trans_type, use_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trans_by_type = bq.Query(use_case_2.trans_by_type).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trans_by_type_pivoted = trans_by_type.pivot('trans_type', 'use_case')\n",
    "trans_by_type_pivoted = trans_by_type_pivoted.fillna(0)\n",
    "trans_by_type_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = trans_by_type_pivoted.plot(kind='bar', figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which effectively shows us that there are a lot more instances of Green, than Yellow, than Red.  This is physically true but doesn't advance our hypothesis so let's scale the data so everything is a fraction of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trans_by_type_pivoted = trans_by_type_pivoted.divide(trans_by_type_pivoted.sum(), axis=1)\n",
    "trans_by_type_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = trans_by_type_pivoted.plot(kind='bar', figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see some signal in the data, good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll stay with our theme of simplicity.  Our model will be the following:\n",
    "\n",
    "* Sample the transaction data, take only the credit events, and take equal populations of Green, Yellow and Red.\n",
    "* For each account in the sampled data and ransform the data from rows in sequential order, to columns in the same order.  I.e.\n",
    "    \n",
    "||||\n",
    "|---|---|---|\n",
    "|1|ACH|100|\n",
    "|2|Check|50|\n",
    "|3|Wire|25|\n",
    "    \n",
    "   becomes,\n",
    "    \n",
    "||||\n",
    "|---|---|---|\n",
    "|ACH|Check|Wire|\n",
    "\n",
    "* For each row, encode the data one-hot.  I.e. with three options ACH, Check, Wire, each option is encoded as a vector of size 3, e.g.\n",
    "\n",
    "|||||\n",
    "|---|---|---|---|\n",
    "|ACH|0|0|1|\n",
    "|Check|0|1|0|\n",
    "|Wire|1|0|0|\n",
    "\n",
    "So, effectively we'll look at time-ordered sequences of credit actions and see if we can learn patterns from them that indicate fraud.\n",
    "\n",
    "We'll focus on two things here:\n",
    "\n",
    "1. The length of the time-ordered sequence.  This is super-important because the aim here is to identify fraud as early as possible.  If our model does a good job of classifying fraud given 5 years of data that's fraud that's occured for 5 years.  If our model does a good job of classifying fraud given 6-12 months of data that's a vast improvement.\n",
    "1. The type of model - RandomForest versus Feed Forward Neural Net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. Challenge everything!  This is one approach to this modelling exercise.  It's a signpost in the right direction but should be regarded as incomplete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql --module use_case_2\n",
    "\n",
    "DEFINE QUERY sample_trans\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  [CreditCardv2.cc_trans_02052016]\n",
    "WHERE\n",
    "  USE_CASE = $use_case AND\n",
    "  CREDIT_DEBIT = 'C'\n",
    "ORDER BY\n",
    "  ACCOUNTID, TRANSACTION_DATE\n",
    "LIMIT\n",
    "  $sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 1000000\n",
    "\n",
    "GREEN_USE_CASE = 'Use Case 2 - Green'\n",
    "YELLOW_USE_CASE = 'Use Case 2 - Yellow'\n",
    "RED_USE_CASE = 'Use Case 2 - Red'\n",
    "\n",
    "GREEN = 'Green'\n",
    "YELLOW = 'Yellow'\n",
    "RED = 'Red'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function transforms our DataFrame from a row per transaction to a column per credit event type.\n",
    "# This introduces some NaN elements so it also transforms those to 'Missing'\n",
    "# TRANSACTION_DATE is included to allow us to later combine and sort different samples together.\n",
    "\n",
    "def widen_trans_per_account(sample_trans, use_case, width):\n",
    "  \n",
    "  sample_trans_acct_ids = sample_trans.ACCOUNTID.drop_duplicates()\n",
    "\n",
    "  sample_trans_wide = pd.DataFrame()\n",
    "\n",
    "  for sample_trans_acct_id in sample_trans_acct_ids:\n",
    "    sample_trans_acct = sample_trans[sample_trans.ACCOUNTID == sample_trans_acct_id]\n",
    "    tmp = pd.DataFrame()\n",
    "    for i in range(width):\n",
    "      tmp['shift_' + str(i)] = sample_trans_acct.TRANSACTION_TYPE.shift(i)\n",
    "    sample_trans_wide = pd.concat([sample_trans_wide, tmp])\n",
    "\n",
    "  sample_trans_wide['USE_CASE'] = use_case\n",
    "  sample_trans_wide['TRANSACTION_DATE'] = sample_trans.TRANSACTION_DATE\n",
    "  \n",
    "  sample_trans_wide = sample_trans_wide.fillna('Missing')\n",
    "  \n",
    "  return sample_trans_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source the Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_trans_green = bq.Query(use_case_2.sample_trans, use_case = GREEN_USE_CASE, sample_size=SAMPLE_SIZE).to_dataframe()\n",
    "sample_trans_green[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_trans_yellow = bq.Query(use_case_2.sample_trans, use_case = YELLOW_USE_CASE, sample_size=SAMPLE_SIZE).to_dataframe()\n",
    "sample_trans_yellow[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_trans_red = bq.Query(use_case_2.sample_trans, use_case=RED_USE_CASE, sample_size=SAMPLE_SIZE).to_dataframe()\n",
    "sample_trans_red[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the Data to Time-Ordered Sequences and Rock and Roll V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WIDTH = 12 # Look at the last 12 payments..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_trans_green_wide = widen_trans_per_account(sample_trans_green, GREEN, WIDTH) \n",
    "\n",
    "print sample_trans_green_wide.shape\n",
    "print\n",
    "print sample_trans_green_wide.describe()\n",
    "print\n",
    "sample_trans_green_wide[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_trans_yellow_wide = widen_trans_per_account(sample_trans_yellow, YELLOW, WIDTH)\n",
    "\n",
    "print sample_trans_yellow_wide.shape\n",
    "print\n",
    "print sample_trans_yellow_wide.describe()\n",
    "print\n",
    "sample_trans_yellow_wide[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_trans_red_wide = widen_trans_per_account(sample_trans_red, RED, WIDTH)\n",
    "\n",
    "print sample_trans_red_wide.shape\n",
    "print\n",
    "print sample_trans_red_wide.describe()\n",
    "print\n",
    "sample_trans_red_wide[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_trans_wide = pd.concat([sample_trans_green_wide, sample_trans_yellow_wide, sample_trans_red_wide])\n",
    "sample_trans_wide = sample_trans_wide.sort_values('TRANSACTION_DATE')\n",
    "sample_trans_wide = sample_trans_wide.drop('TRANSACTION_DATE', 1)\n",
    "\n",
    "print sample_trans_wide.describe()\n",
    "print\n",
    "sample_trans_wide[:3]\n",
    "#why/how did we merge all Credit types in shift_0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shift_encoder = preprocessing.LabelEncoder()\n",
    "shift_encoder.fit(sample_trans_wide.shift_1.drop_duplicates())\n",
    "\n",
    "use_case_encoder = preprocessing.LabelEncoder()\n",
    "use_case_encoder.fit(sample_trans_wide.USE_CASE.drop_duplicates())\n",
    "\n",
    "sample_trans_wide_enc = pd.DataFrame(index=sample_trans_wide.index)\n",
    "\n",
    "sample_trans_wide_enc['USE_CASE'] = use_case_encoder.transform(sample_trans_wide.USE_CASE)\n",
    "\n",
    "for i in range(WIDTH):\n",
    "  sample_trans_wide_enc['shift_' + str(i) + '_enc'] = shift_encoder.transform(sample_trans_wide['shift_' + str(i)])\n",
    "\n",
    "sample_trans_wide_enc[:5]\n",
    "#does the encoder automatically assign values 0-6? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = sample_trans_wide_enc[sample_trans_wide_enc.columns[1:]]\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = sample_trans_wide_enc[sample_trans_wide_enc.columns[0]]\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_enc = pd.DataFrame()\n",
    "for i in range(WIDTH):\n",
    "  enc = preprocessing.OneHotEncoder()\n",
    "  enc.fit(X['shift_' + str(i) + '_enc'].values.reshape((-1,1)))\n",
    "  X_enc = pd.concat([X_enc, pd.DataFrame(enc.transform(X['shift_' + str(i) + '_enc'].values.reshape((-1,1))).toarray())], axis=1)\n",
    "\n",
    "X_enc.columns = [i for i in range(len(X_enc.columns))]\n",
    "X_enc[:5]\n",
    "#does this code block vectorize the encoded data frames? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_split = int(len(X_enc) * 0.8)\n",
    "X_train = X_enc[:X_split]\n",
    "X_test = X_enc[X_split:]\n",
    "\n",
    "y_split = int(len(y) * 0.8)\n",
    "y_train = y[:y_split]\n",
    "y_test = y[y_split:]\n",
    "\n",
    "print X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "#splitting training and testing datasets to 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators = 25)\n",
    "#why 25 trees? \n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print accuracy_score(y_train, classifier.predict(X_train))\n",
    "print accuracy_score(y_test, classifier.predict(X_test))\n",
    "print\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "print metrics.classification_report(y_test, y_pred)\n",
    "print\n",
    "print metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model(X, y):\n",
    "  layers = skflow.ops.dnn(X, [25], keep_prob=0.5)\n",
    "  return skflow.models.logistic_regression(layers, y)\n",
    "\n",
    "classifier = skflow.TensorFlowEstimator(\n",
    "  model_fn=model, n_classes=3, steps=10000, learning_rate=0.003, optimizer='Adam')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print accuracy_score(y_train, classifier.predict(X_train))\n",
    "print accuracy_score(y_test, classifier.predict(X_test))\n",
    "print\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "print metrics.classification_report(y_test, y_pred)\n",
    "print\n",
    "print metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the Data to Time-Ordered Sequences and Rock and Roll V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WIDTH = 24 # Look at the last 24 payments...\n",
    "\n",
    "sample_trans_green_wide = widen_trans_per_account(sample_trans_green, GREEN, WIDTH)\n",
    "\n",
    "print sample_trans_green_wide.shape\n",
    "print\n",
    "print sample_trans_green_wide.describe()\n",
    "print\n",
    "print sample_trans_green_wide[:3]\n",
    "print\n",
    "\n",
    "sample_trans_yellow_wide = widen_trans_per_account(sample_trans_yellow, YELLOW, WIDTH)\n",
    "\n",
    "print sample_trans_yellow_wide.shape\n",
    "print\n",
    "print sample_trans_yellow_wide.describe()\n",
    "print\n",
    "print sample_trans_yellow_wide[:3]\n",
    "print\n",
    "\n",
    "sample_trans_red_wide = widen_trans_per_account(sample_trans_red, RED, WIDTH)\n",
    "\n",
    "print sample_trans_red_wide.shape\n",
    "print\n",
    "print sample_trans_red_wide.describe()\n",
    "print\n",
    "print sample_trans_red_wide[:3]\n",
    "print\n",
    "\n",
    "sample_trans_wide = pd.concat([sample_trans_green_wide, sample_trans_yellow_wide, sample_trans_red_wide])\n",
    "sample_trans_wide = sample_trans_wide.sort_values('TRANSACTION_DATE')\n",
    "sample_trans_wide = sample_trans_wide.drop('TRANSACTION_DATE', 1)\n",
    "\n",
    "print sample_trans_wide.describe()\n",
    "print\n",
    "print sample_trans_wide[:3]\n",
    "print\n",
    "\n",
    "shift_encoder = preprocessing.LabelEncoder()\n",
    "shift_encoder.fit(sample_trans_wide.shift_1.drop_duplicates())\n",
    "\n",
    "use_case_encoder = preprocessing.LabelEncoder()\n",
    "use_case_encoder.fit(sample_trans_wide.USE_CASE.drop_duplicates())\n",
    "\n",
    "sample_trans_wide_enc = pd.DataFrame(index=sample_trans_wide.index)\n",
    "\n",
    "sample_trans_wide_enc['USE_CASE'] = use_case_encoder.transform(sample_trans_wide.USE_CASE)\n",
    "\n",
    "for i in range(WIDTH):\n",
    "  sample_trans_wide_enc['shift_' + str(i) + '_enc'] = shift_encoder.transform(sample_trans_wide['shift_' + str(i)])\n",
    "\n",
    "print sample_trans_wide_enc[:5]\n",
    "print \n",
    "\n",
    "X = sample_trans_wide_enc[sample_trans_wide_enc.columns[1:]]\n",
    "print X[:5]\n",
    "print \n",
    "\n",
    "y = sample_trans_wide_enc[sample_trans_wide_enc.columns[0]]\n",
    "print y[:5]\n",
    "print\n",
    "\n",
    "X_enc = pd.DataFrame()\n",
    "for i in range(WIDTH):\n",
    "  enc = preprocessing.OneHotEncoder()\n",
    "  enc.fit(X['shift_' + str(i) + '_enc'].values.reshape((-1,1)))\n",
    "  X_enc = pd.concat([X_enc, pd.DataFrame(enc.transform(X['shift_' + str(i) + '_enc'].values.reshape((-1,1))).toarray())], axis=1)\n",
    "\n",
    "X_enc.columns = [i for i in range(len(X_enc.columns))]\n",
    "print X_enc[:5]\n",
    "print\n",
    "\n",
    "X_split = int(len(X_enc) * 0.8)\n",
    "X_train = X_enc[:X_split]\n",
    "X_test = X_enc[X_split:]\n",
    "\n",
    "y_split = int(len(y) * 0.8)\n",
    "y_train = y[:y_split]\n",
    "y_test = y[y_split:]\n",
    "\n",
    "print X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators = 50)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print accuracy_score(y_train, classifier.predict(X_train))\n",
    "print accuracy_score(y_test, classifier.predict(X_test))\n",
    "print\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "print metrics.classification_report(y_test, y_pred)\n",
    "print\n",
    "print metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model(X, y):\n",
    "  layers = skflow.ops.dnn(X, [50], keep_prob=0.5)\n",
    "  return skflow.models.logistic_regression(layers, y)\n",
    "\n",
    "classifier = skflow.TensorFlowEstimator(\n",
    "  model_fn=model, n_classes=3, steps=10000, learning_rate=0.003, optimizer='Adam')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print accuracy_score(y_train, classifier.predict(X_train))\n",
    "print accuracy_score(y_test, classifier.predict(X_test))\n",
    "print\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "print metrics.classification_report(y_test, y_pred)\n",
    "print\n",
    "print metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the Data to Time-Ordered Sequences and Rock and Roll V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WIDTH = 60 # Look at the last 60 payments...\n",
    "\n",
    "sample_trans_green_wide = widen_trans_per_account(sample_trans_green, GREEN, WIDTH)\n",
    "\n",
    "print sample_trans_green_wide.shape\n",
    "print\n",
    "print sample_trans_green_wide.describe()\n",
    "print\n",
    "print sample_trans_green_wide[:3]\n",
    "print\n",
    "\n",
    "sample_trans_yellow_wide = widen_trans_per_account(sample_trans_yellow, YELLOW, WIDTH)\n",
    "\n",
    "print sample_trans_yellow_wide.shape\n",
    "print\n",
    "print sample_trans_yellow_wide.describe()\n",
    "print\n",
    "print sample_trans_yellow_wide[:3]\n",
    "print\n",
    "\n",
    "sample_trans_red_wide = widen_trans_per_account(sample_trans_red, RED, WIDTH)\n",
    "\n",
    "print sample_trans_red_wide.shape\n",
    "print\n",
    "print sample_trans_red_wide.describe()\n",
    "print\n",
    "print sample_trans_red_wide[:3]\n",
    "print\n",
    "\n",
    "sample_trans_wide = pd.concat([sample_trans_green_wide, sample_trans_yellow_wide, sample_trans_red_wide])\n",
    "sample_trans_wide = sample_trans_wide.sort_values('TRANSACTION_DATE')\n",
    "sample_trans_wide = sample_trans_wide.drop('TRANSACTION_DATE', 1)\n",
    "\n",
    "print sample_trans_wide.describe()\n",
    "print\n",
    "print sample_trans_wide[:3]\n",
    "print\n",
    "\n",
    "shift_encoder = preprocessing.LabelEncoder()\n",
    "shift_encoder.fit(sample_trans_wide.shift_1.drop_duplicates())\n",
    "\n",
    "use_case_encoder = preprocessing.LabelEncoder()\n",
    "use_case_encoder.fit(sample_trans_wide.USE_CASE.drop_duplicates())\n",
    "\n",
    "sample_trans_wide_enc = pd.DataFrame(index=sample_trans_wide.index)\n",
    "\n",
    "sample_trans_wide_enc['USE_CASE'] = use_case_encoder.transform(sample_trans_wide.USE_CASE)\n",
    "\n",
    "for i in range(WIDTH):\n",
    "  sample_trans_wide_enc['shift_' + str(i) + '_enc'] = shift_encoder.transform(sample_trans_wide['shift_' + str(i)])\n",
    "\n",
    "print sample_trans_wide_enc[:5]\n",
    "print \n",
    "\n",
    "X = sample_trans_wide_enc[sample_trans_wide_enc.columns[1:]]\n",
    "print X[:5]\n",
    "print \n",
    "\n",
    "y = sample_trans_wide_enc[sample_trans_wide_enc.columns[0]]\n",
    "print y[:5]\n",
    "print\n",
    "\n",
    "X_enc = pd.DataFrame()\n",
    "for i in range(WIDTH):\n",
    "  enc = preprocessing.OneHotEncoder()\n",
    "  enc.fit(X['shift_' + str(i) + '_enc'].values.reshape((-1,1)))\n",
    "  X_enc = pd.concat([X_enc, pd.DataFrame(enc.transform(X['shift_' + str(i) + '_enc'].values.reshape((-1,1))).toarray())], axis=1)\n",
    "\n",
    "X_enc.columns = [i for i in range(len(X_enc.columns))]\n",
    "print X_enc[:5]\n",
    "print\n",
    "\n",
    "X_split = int(len(X_enc) * 0.8)\n",
    "X_train = X_enc[:X_split]\n",
    "X_test = X_enc[X_split:]\n",
    "\n",
    "y_split = int(len(y) * 0.8)\n",
    "y_train = y[:y_split]\n",
    "y_test = y[y_split:]\n",
    "\n",
    "print X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators = 250)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print accuracy_score(y_train, classifier.predict(X_train))\n",
    "print accuracy_score(y_test, classifier.predict(X_test))\n",
    "print\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "print metrics.classification_report(y_test, y_pred)\n",
    "print\n",
    "print metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model(X, y):\n",
    "  layers = skflow.ops.dnn(X, [400], keep_prob=0.5)\n",
    "  return skflow.models.logistic_regression(layers, y)\n",
    "\n",
    "classifier = skflow.TensorFlowEstimator(\n",
    "  model_fn=model, n_classes=3, steps=10000, learning_rate=0.003, optimizer='Adam')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print accuracy_score(y_train, classifier.predict(X_train))\n",
    "print accuracy_score(y_test, classifier.predict(X_test))\n",
    "print\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "print metrics.classification_report(y_test, y_pred)\n",
    "print\n",
    "print metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the Data to Time-Ordered Sequences and Rock and Roll V4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WIDTH = 100 # Look at the last 100 payments...\n",
    "\n",
    "sample_trans_green_wide = widen_trans_per_account(sample_trans_green, GREEN, WIDTH)\n",
    "\n",
    "print sample_trans_green_wide.shape\n",
    "print\n",
    "print sample_trans_green_wide.describe()\n",
    "print\n",
    "print sample_trans_green_wide[:3]\n",
    "print\n",
    "\n",
    "sample_trans_yellow_wide = widen_trans_per_account(sample_trans_yellow, YELLOW, WIDTH)\n",
    "\n",
    "print sample_trans_yellow_wide.shape\n",
    "print\n",
    "print sample_trans_yellow_wide.describe()\n",
    "print\n",
    "print sample_trans_yellow_wide[:3]\n",
    "print\n",
    "\n",
    "sample_trans_red_wide = widen_trans_per_account(sample_trans_red, RED, WIDTH)\n",
    "\n",
    "print sample_trans_red_wide.shape\n",
    "print\n",
    "print sample_trans_red_wide.describe()\n",
    "print\n",
    "print sample_trans_red_wide[:3]\n",
    "print\n",
    "\n",
    "sample_trans_wide = pd.concat([sample_trans_green_wide, sample_trans_yellow_wide, sample_trans_red_wide])\n",
    "sample_trans_wide = sample_trans_wide.sort_values('TRANSACTION_DATE')\n",
    "sample_trans_wide = sample_trans_wide.drop('TRANSACTION_DATE', 1)\n",
    "\n",
    "print sample_trans_wide.describe()\n",
    "print\n",
    "print sample_trans_wide[:3]\n",
    "print\n",
    "\n",
    "shift_encoder = preprocessing.LabelEncoder()\n",
    "shift_encoder.fit(sample_trans_wide.shift_1.drop_duplicates())\n",
    "\n",
    "use_case_encoder = preprocessing.LabelEncoder()\n",
    "use_case_encoder.fit(sample_trans_wide.USE_CASE.drop_duplicates())\n",
    "\n",
    "sample_trans_wide_enc = pd.DataFrame(index=sample_trans_wide.index)\n",
    "\n",
    "sample_trans_wide_enc['USE_CASE'] = use_case_encoder.transform(sample_trans_wide.USE_CASE)\n",
    "\n",
    "for i in range(WIDTH):\n",
    "  sample_trans_wide_enc['shift_' + str(i) + '_enc'] = shift_encoder.transform(sample_trans_wide['shift_' + str(i)])\n",
    "\n",
    "print sample_trans_wide_enc[:5]\n",
    "print \n",
    "\n",
    "X = sample_trans_wide_enc[sample_trans_wide_enc.columns[1:]]\n",
    "print X[:5]\n",
    "print \n",
    "\n",
    "y = sample_trans_wide_enc[sample_trans_wide_enc.columns[0]]\n",
    "print y[:5]\n",
    "print\n",
    "\n",
    "X_enc = pd.DataFrame()\n",
    "for i in range(WIDTH):\n",
    "  enc = preprocessing.OneHotEncoder()\n",
    "  enc.fit(X['shift_' + str(i) + '_enc'].values.reshape((-1,1)))\n",
    "  X_enc = pd.concat([X_enc, pd.DataFrame(enc.transform(X['shift_' + str(i) + '_enc'].values.reshape((-1,1))).toarray())], axis=1)\n",
    "\n",
    "X_enc.columns = [i for i in range(len(X_enc.columns))]\n",
    "print X_enc[:5]\n",
    "print\n",
    "\n",
    "X_split = int(len(X_enc) * 0.8)\n",
    "X_train = X_enc[:X_split]\n",
    "X_test = X_enc[X_split:]\n",
    "\n",
    "y_split = int(len(y) * 0.8)\n",
    "y_train = y[:y_split]\n",
    "y_test = y[y_split:]\n",
    "\n",
    "print X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators = 250)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print accuracy_score(y_train, classifier.predict(X_train))\n",
    "print accuracy_score(y_test, classifier.predict(X_test))\n",
    "print\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "print metrics.classification_report(y_test, y_pred)\n",
    "print\n",
    "print metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model(X, y):\n",
    "  layers = skflow.ops.dnn(X, [1600], keep_prob=0.5)\n",
    "  return skflow.models.logistic_regression(layers, y)\n",
    "\n",
    "classifier = skflow.TensorFlowEstimator(\n",
    "  model_fn=model, n_classes=3, steps=10000, learning_rate=0.003, optimizer='Adam')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print accuracy_score(y_train, classifier.predict(X_train))\n",
    "print accuracy_score(y_test, classifier.predict(X_test))\n",
    "print\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "print metrics.classification_report(y_test, y_pred)\n",
    "print\n",
    "print metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
